#白话/gi机器学习/gi算法/gi（/w十一/nz）/w GMM/nz
就是/v高斯混合模型/gi，/w用/pgmm/nz去/vf聚类/gi的话/udh，/w就/d变成/v了/ule一个然/nz估计/gi的/ude1问题/gi，/w估计/gi的/ude1参数/gi就是/v选取/gi每个/r高斯/nrf部件/n的/ude1概率/gi，/w每个/r高斯/nrf各自/rr的/ude1均方差/nz；/w我们/rr可以/v把/pba实际/n数据/gi看做/v由/p这个/rzgmm/nz随机/b数/n生成器/n产生/v的/ude1，/wn/nz个/q数据/gi就是/vn/nz个/q观测/vn，/w数据/gi之间/f独立/a；/w根据/p数据/gi找出/v这个/rz模型/gi的/ude1参数/gi，/w有/vyou了/ule模型/gi参数/gi，/w我/rr就/d能/v简单/a的/ude1算出/vf数据/gi属于/v哪个/ry高斯/nrf部件/n的/ude1概率/gi最大/gm，/w论文/gi中/f一般/ad都/d说/vresponsibility/nz，/w所以/c用/p高斯混合模型/gi来/vf聚类/gi，/w实质/gi就是/v一个/mq参数估计/gm问题/gi，/wem/nz算法/gi就是/v专门/d干/v这个/rz的/ude1；/whmm/nz也/d是/vshi一个/mq参数估计/gm问题/gi，/w而且/chmm/nz跟/pgmm/nz有/vyou很大/d的/ude1相/d；/w总的/b来说/uls，/w基于/p概率/gi的/ude1聚类/gi /x用/p到/v的/ude1就是/v两/nz个/q东西/n map/nz（/w比如/v贝叶斯/nrf）/w mle/nz（/w这个/rz就/d很/d多/a了/ule，/wgmm gtm/nz都/d是/vshi这样/rzv的/ude1）/w,/nz；/w其他/rzv的/ude1还有/vme/nz（/w最大/gm熵/n）/w，/wica/gi的/ude1一种/nz方法/gi就是/v基于/p最大/gm熵/n模型/gi得出/v统计/gi独立/a的/ude1各个/rz分量/n；/w关于/pem/nz算法/gi，/w以后/f专门/d详细/gi讨论/gi /x./nz如果/c将/dgmm/nz稍微/d推广/gi下/f，/w加上/v一点/m约束/gi，/w就是/v假定/v这些/rz高斯/nrf中心/gi位于/v高维/nz空间/n的/ude1一个/mq流形/gp上/f，/w就/d可以/v把/pba这些/rz高斯/nrf中心/gi映射/gi到/v一个/mq低/a维隐/nz空间/n，/w我/rr就/d可以/v将/d实际/n的/ude1数据/gi通过/p后验/nz概率/gi映射/gi到/v那个/rz隐/v空间坐标/gm上/f，/w实现/gi数据/gi降维/nz，/w或者/c数据可视化/gi，/w这/rzv就是/vgtm/nz算法/gi，/w这个/rz算法/gi可以/v在/p低维/nz空间/n保持数据/n的/ude1拓扑/n有序/gi，/w但是/c这个/rz保证/v需要/v实际/n数据/gi满足/v上面/f的/ude1那个/rz假设/gi，/w就是/v假定/v这些/rz高斯/nrf中心/gi位于/v高维/nz空间/n的/ude1一个/mq流形/gp上/f，/w隐/v空间/n的/ude1维度/gi是/vshi这个/rz流形/gp的/ude1本质/n维度/gi才/d行/ng，/w如果/c不/d满足/v，/w就/d不能/v完全/ad保证/v这些/rz数据/gi的/ude1拓扑/n有序/gi；/w