#浅/a说/v机器学习/gi
我/rr概念/gi里面/f机器学习/gi算法/gi是/vshi这样/rzv一个/mq步骤/gi：/w1/nz）/w对于/p一个/mq问题/gi，/w我们/rr用/p数学/gi语言/gi来/vf描述/gi它/rr，/w然后/c建立/gi一个/mq模型/gi，/w例如/v回归/gi模型/gi或者/c分类/gi模型/gi等/udeng来/vf描述/gi这个/rz问题/gi；/w2/nz）/w通过/p最大/gm然/rz、/w最大/gm后/f验/v概率/gi或者/c最小化/nz分类/gi误差/gi等等/udeng建立/gi模型/gi的/ude1代价/gi函数/gi，/w也/d就是/v一个/mq最优化/gm问题/gi。/w找到/v最优化/gm问题/gi的/ude1解/v，/w也/d就是/v能/v拟合/gi我们/rr的/ude1数据/gi的/ude1最好/d的/ude1模型/gi参数/gi；/w3/nz）/w然后/c我们/rr需要/v求解/gi这个/rz代价/gi函数/gi，/w找到/v最优解/gi。/w这/rzv求解/gi也/d就/d分/qt很/d多/a种/q情况/n了/ule：/w   a/nz）/w如果/c这个/rz优化/gi函数/gi存在/v解析/gi解/v。/w例如/v我们/rr求/v最/d一般/ad是/vshi对/p代价/gi函数/gi求导/nz，/w找到/v导数/n为/p0/nz的/ude1点/gi，/w也/d就是/v最大/gm或者/c最小/a的/ude1地方/n了/ule。/w如果/c代价/gi函数/gi能/v简单/a求导/nz，/w并且/c求导/nz后/f为/p0/nz的/ude1式/k子/ng存在/v解析解/nz，/w那么/c我们/rr就/d可以/v直接/ad得到/v最优/ad的/ude1参数/gi了/ule。/w   b/nz）/w如果/c式/k子/ng很难/d求导/nz，/w例如/v函数/gi里面/f存在/v隐含/v的/ude1变量/gi或者/c变量/gi相互间/nz存在/v耦合/vn，/w也/d就/d互相/d依赖/v的/ude1情况/n。/w或者/c求导/nz后/f式/k子/ng得不到/v解释/gi解/v，/w例如/v未知/v参数/gi的/ude1个数/n大于/v已知/nz方程/gi组/n的/ude1个数/n等/udeng。/w这时候/rzt我们/rr就/d需要/v借助/v迭代/gi算法/gi来/vf一步一步/nz找到/v最/d有/vyou解/v了/ule。/w迭代/gi是/vshi个/q很/d神奇/a的/ude1东西/n，/w它/rr将/d远大/a的/ude1目标/gi记/v在/p心/n上/f，/w然后/c给/p自己/rr定/v个/q短期/b目标/gi一般/ad这个/rz时候/n就是/v求导/nz通过/p梯度/gi上升/vi的/ude1方法/gi   /x另外/c需要/v考虑/v的/ude1情况/n是/vshi，/w如果/c代价/gi函数/gi是/vshi凸函数/gm，/w那么/c就/d存在/v全局/n最优解/gi，/w相当于/v只有/c一个/mq山峰/n，/w那/rzv命中注定/nz了/ule，/w它/rr就是/v你/rr要/v找/v的/ude1唯一/b了/ule。/w但/c如果/c是非/n凸/ag的/ude1，/w那么/c就/d会/v有/vyou很/d多/a局部/n最优/ad的/ude1解/v。/w