#模型/gi融合/v
  /x我/rr对此/d的/ude1理解/gi还/d比较/gi浅显/a，/w最早/d知道/v这个/rz东西/n还是/c今年/t看到/v了/ule一个/mq，/w主要/b理解/gi就是/v单个/b模型/gi的/ude1结果/n不够/a理想/n，/w如果/c想得到/vl更好/d的/ude1结果/n，/w需要/v把/pba很/d多/a单个/b模型/gi的/ude1结果/n融合/v在/p一起/s。/w  /x那么/c究竟/d是/vshi怎么/ryv个/q“/w融合/v”/w呢/y？/w /x这个/rz问题/gi之前/f一直/d没/d搞/v明白/v，/w最近/t在用/b随机/b森林/gi方面/n的/ude1方法/gi来/vf处理/vn数据/gi，/w顿悟/vi出/vf，/w模型/gi融合/v可以/v想像/v成“/nz再次/d的/ude1机器学习/gi过程/gi”/w。/w已知/nz模型/gia/nz、/w模型/gib/nz、/w模型/gic/nz，/w测试数据/gi输出/gi的/ude1结果/n分别为/va/nz，/wb/nz和/ccc/nz，/w用/p训练数据/gi通过/p模型/gi输出/gi的/ude1训练数据/gi预测/gi结果/n为/pa/nz1/nz，/wb/nz1/nz和/ccc/nz1。/nz我们/rr可以/v将/da/nz1/nz，/wb/nz1/nz，/wc1/gi的/ude1数据/gi带入/v到/v训练数据/gi中/f再次/d训练/gi，/w得到/v更加/d贴近/v真实/a结果/n的/ude1模型/gi，/w再/d用/p这个/rz模型/gi训练/gi测试数据/gi结果/na/nz，/wb/nz和/ccc/nz。/w得到/v的/ude1结果/n会/v比/p一次/nz训练/gi的/ude1好/a。/w（/w这/rzv其实/d也/d就是/v个/q多/a层/gi神经网络/nz的/ude1思想/gi，/w但是/c与/cc神经网络/nz不同/a的/ude1是/vshi：/w每次/r的/ude1输入/v数据/gi是/vshi“/w上次/t的/ude1输入/v数据/gi”“/nz上次/t的/ude1输入/v结果/n”/w）/w。/w  /x当然/d，/w方法/gi多种多样/bl，/w“/w上次/t的/ude1输入/v数据/gi”“/nz上次/t的/ude1输入/v结果/n”/w是/vshi一种/nz输入/v数据类型/gi，/w“/w上次/t的/ude1输入/v结果/n”/w也/d是/vshi一种/nz输入/v数据类型/gi，/w融合/v所用/b的/ude1方法/gi也/d很/d多/a，/w直接/ad用/p机器学习/gi里/f的/ude1各类/r方法/gi，/w或者/c用/p统计/gi回归/gi，/w皆/d可/v。/w一切/rz皆/d有可能/nz。/w