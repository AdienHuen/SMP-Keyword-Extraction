#理解/giGBDT/nz算法/gi（/w二/nz）/w―/w―/w基于/p残差/nz的/ude1版本/n
gbdt/gi算法/gi有/vyou两/nz种/q描述/gi思路/gi，/w一个/mq是/vshi基于/p残差/nz的/ude1版本/n，/w一个/mq是/vshi基于/p梯度/gigradient/nz的/ude1版本/n。/w这/rzv篇/q我们/rr先/d说说/v基于/p残差/nz的/ude1版本/n。/w前面/f一篇/nz博文/nz已经/d说/v了/ule该/rz版本/n的/ude1大致/d原理/gi，/w请/v参考/gi。/w /x这/rzv篇/q我们/rr再/d总结一个/nz几个/nz注意/v点/gi：/w这个/rz版本/n的/ude1核心/n思路/gi：/w每个/r回归/gi树/gi学习/gi前面/f树/gi的/ude1残差/nz，/w并且/c用/pshrinkage/nz把/pba学习/gi到/v的/ude1结果/n大步/d变/v小步/nz，/w不断/d迭代/gi学习/gi。/w其中/rz的/ude1代价/gi函数/gi是/vshi常见/a的/ude1均方差/nz。/w其/rz基本/a做法/n就是/v：/w先/d学习/gi一个/mq回归树/nz，/w然后/c“/w真实/a值-预测/nz值/n*/nzshrinkage/nz”求/nz此时/r的/ude1残差/nz，/w把/pba这个/rz残差/nz作为/p目标值/n，/w学习/gi下一个/nz回归树/nz，/w继续/v求/v残差/nz…/w…/w直到/v建立/gi的/ude1回归树/nz的/ude1数目/gi达到/v一定/b要求/n或者/c残差/nz能够/v容忍/v，/w停止/gi学习/gi。/w我们/rr知道/v，/w残差/nz是/vshi预测值/n和/cc目标值/n的/ude1差值/n，/w这个/rz版本/n是/vshi把/pba残差/nz作为/p全局/n最优/ad的/ude1绝对/d方向/gi来/vf学习/gi。/w这个/rz版本/n更加/d适用/vi于/p回归/gi问题/gi，/w线性/gi和/cc非线性/gi的/ude1均可/v，/w而且/c在/p设定/v了/ule阈值/n之后/f还/d可以/v有/vyou分类/gi的/ude1功能/gi。/w当时/t该/rz版本/n使用/gi残差/nz，/w很难/d处理/vn纯/a回归/gi以外/f的/ude1问题/gi。/w版本/n二中/nz使用/gi梯度/gi，/w只要/c建立/gi的/ude1代价/gi函数/gi能够/v求导/nz，/w那么/c就/d可以/v使用/gi版本二/nz的/ude1gbdt/gi算法/gi，/w例如/vlambdamart/nz学习/gi排序算法/gi。/wshrinkage/nz和/cc梯度下降/gi法/n中/f学习/gi步长/nzalpha/nz的/ude1关系/gi。/wshrinkage/nz设/v小/a了/ule只/d会/v让/v学习/gi更慢/d，/w设/v大/a了/ule就/d等于/v没设/v，/w它/rr适用/vi于/p所有/b增量/gi迭代/gi求解/gi问题/gi；/w而/ccgradient/nz的/ude1步长/nz设小/nz了/ule容易/ad陷入/v局部/n最/d优点/gi，/w设/v大/a了/ule容易/ad不/d收敛/gi。/w它/rr仅/d用于/v用/p梯度下降/gi求解/gi。/w这/rzv两者/rzv其实/d没/d太大/d关系/gi。/w