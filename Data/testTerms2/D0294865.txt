#深度学习/gi中/f梯度下降/gi知识/gi准备/v
考虑/v一个/mq代价/gi函数/gic /nz,/nz /x它/rr根据/p参数/gi向/p量计算/l出/vf当前/t迭代/gi模型/gi的/ude1代价/gi，/w记作/nzc/nz(/nz)/nz./nz /x机器学习/gi中/f，/w我们/rr的/ude1任务/gi就是/v得到/v代价/gi的/ude1最小/a，/w在/p机器学习/gi中/f代价/gi函数/gi通常/d是/vshi损失/n函数/gi的/ude1均/d，/w或者/c是/vshi它/rr的/ude1数学/gi期望/vn。/w见/v下图/n：/w这个/rz叫做/v泛化/gi损失/n，/w在/p监督/gi学/v过程/gi中/f，/w我们/rr知道/vz/nz=/nz(/nzx/nz,/nzy/nz)/nz /x,/nz并且/c f/nz（/wx/nz)/nz /x是/vshi对/py/nz的/ude1预测/gi。/w什么/ry是/vshi这里/rzs的/ude1梯度/gi呢/y？/w当是/d标量/n的/ude1时候/n，/w代价/gi函数/gi的/ude1梯度/gi可/v表示/v如下/vi：/w当/p很小/a的/ude1时候/n，/w它/rr就是/v的/ude1另外/c一种/nz表达/gi，/w而/cc我们/rr就是/v让/v小于/v零/nz，/w且/c越/d小/a越好/d。/w当时/t一个/mq向量/gi的/ude1时候/n，/w代价/gi函数/gi的/ude1 /x梯度/gi也/d是/vshi一个/mq向量/gi，/w每个/r都/d是/vshi一个/mqi/nz,/nz这里/rzs我/rr考虑/v其他/rzv变量/gi是/vshi固定/a的/ude1，/w仅仅/d是/vshi在/p变化/gi。/w当/p很小/a的/ude1时候/n，/w就/d成为/v再/d来看/u梯度下降/gi：/w这里/rzs我们/rr的/ude1是/vshi找到/v一个/mq能够/v最小化/nz代价/gi函数/gi的/ude1。/w若/c果/ng我们/rr能够/v搞定/v这个/rz式/k子/ng，/w我们/rr就/d牛叉/nr大/a了/ule，/w这样/rzv我们/rr就/d找到/v最优/ad的/ude1模型/gi了/ule。/w然而/c理想/n很/d丰满/a，/w现实/n很/d骨感/nz，/w在/p99.99999%/nz的/ude1情况下/nz,/nz我们/rr不/d可能/v找到/v上面/f那个/rz等/udeng式/k的/ude1答案/gi，/w除非/c计算机/gi是/vshi人/n，/w所以/c我们/rr使用/gi数学方法/nz。/w大部分/n方法/gi就是/v本地/gi下降/vi的/ude1想法/gi。/w /x每/rz轮迭/nz代/q调整/vn下/f，/w去/vf减少/v代价/gi函数/gi的/ude1，/w直到/v不能/v下降/vi代价/gi函数/gi为止/u，/w这样/rzv我们/rr达到/v了/ule一个/mq本地/gi最小/a（/w如果/c我们/rr很/d幸运/gi，/w我/rr可能/v找到/v的/ude1就是/v全局/n最小/a）/w。/w最简单/nz的/ude1基于/p梯度/gi的/ude1就是/v梯度下降/gi。/w有/vyou很/d多/a梯度下降/gi的/ude1方法/gi，/w我们/rr这里/rzs说/v的/ude1是/vshi最/d普通/a的/ude1方法/gi。/w这里/rzs代表/nnt第/mqk/nz论/v的/ude1迭代/gi，/w就是/v学习/gi率/v，/w它/rr是/vshi个/q标量/n。/w你/rr可以/v使用/gi固定/a的/ude1学习/gi率/v，/w也/d可以/v使用/gi适配/nz的/ude1学习/gi率/v，/w此处/s不再/d详表/nz。/w我们/rr再/d来看/u一个/mq随机/b梯度下降/gi法/n(/nzstochastic gradient descent/nz)/nz,/nz简称/vsgd/nz：/w我们/rr知道/vc/nz是/vshi个/q均/d，/w一般/ad和/cc具体/a的/ude1样本/gi无关/v，/w如果/c我们/rr对/p的/ude1更快一点/nz，/w在/p极端/n情况下/nz，/w每个/r样本/gi更新/gi一次/nz，/w我们/rr有/vyou下面/f的/ude1公式/gi：/w，/w这里/rzsz/nz是/vshi下一个/nz样本/gi，/w或者/c是/vshi在线/vn学习/gi的/ude1下一个/nz输入/v信号/gi。/wsgd/nz是/vshi一个/mq更加/d通用/gi的/ude1原则/gi，/w它/rr的/ude1梯度下降/gi方向/gi比较/gi随机/b，/w换句话说/c，/w是/vshi随着/p兴趣/gi的/ude1方向/gi下降/vi的/ude1。/wsgd/nz和/cc普通/a的/ude1梯度下降/gi基本/a类/gi，/w只不过/d它/rr增加/v了/ule不/d少/a随机性/n。/wsgd/nz比/p普通/a的/ude1梯度下降/gi（/w批量/d梯度下降/gi）/w更快/d，/w因为/c它/rr权/n向量/gi更/d频繁/a。/w这个/rz对于/p集/q比较/gi大/a的/ude1时候/n，/w或者/c是/vshi在线/vn学习/gi比较/gi有/vyou用/p。/w事实上/bl，/w在/p机器学习/gi任务/gi中/f，/w大家/rr仅仅/d在/p代价/gi函数/gi无法/v分解/gi为/p上面/f的/ude1式子/n的/ude1时候/n才/d使用/gi批量/d梯度下降/gi法/n。/w小批量/n梯度下降/gi法/n：/w这个/rz不想/v仔细/ad讲/v了/ule，/w就是说/c每/rz训练/gi10/nz个/q或者/c20/nz个/q下降/vi一次/nz，/w方法/gi和/cc批/q的/ude1那个/rz差不多/al。/w均线/n法/n法/n：/w我们/rr不/d使用/gi当前/t的/ude1样本/gi的/ude1梯度/gi，/w而是/c计算/gi一个/mq过去/vf样本/gi的/ude1均线/n，/w然后/c用/p均线/n去/vf做权/nz向量/gi的/ude1调整/vn。/w――――――――――-最后/nz华丽/a的/ude1分割线/nz―/w―/w―/w―/w―/w―/w―/w―/w―/w―/w―/w―/w―/w―/w本/rz教程/gi的/ude1目录/gi博文/nz请/v点击/v这里/rzs。/w如果/c大家/rr想/v先/d对/p机器学习/gi进行/vn入门/gi了解/v，/w可/v参看/v这里/rzs的/ude1简单/a介绍/gi。/w如果/c需要/v简单/a了解/v深度学习/gi的/ude1内容/gi，/w可/v参看/v这里/rzs简单/a的/ude1介绍/gi。/w学习/gi这些/rz教程/gi之前/f，/w可以/v先/d热身/vn下/f，/w这里/rzs是/vshitheano/nz的/ude1基础教程/nz，/w学/v完/vi之后/f，/w再/d看下/v这个/rz东东/ns，/w里面/f有/vyou一/nz些/q基本/a的/ude1概念/gi和/cc一/nz些/q测试/gi的/ude1训练/gi集/q。/w原文/n地址/gi：/w