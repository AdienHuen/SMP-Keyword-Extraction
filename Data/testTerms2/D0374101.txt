#名词/n解析/gi之/uzhi泛化/gi误差/gi
摘要/n：/w以前/f在/p机器学习/gi中/f一直/d使用/gi经验/gi风险/gi来/vf近/a真实/a风险/gi，/w但是/c事实上/bl大/a多数/a情况/n经验/gi风险/gi并不/d能够/v准确/a近/a真实/a风险/gi。/w后来/t业界/n就/d提出/v了/ule泛化/gi误差/gi的/ude1概念/gi（/wgeneralization error/nz）/w,/nz在/p机器学习/gi中/f泛化/gi误差/gi是/vshi用来/v衡量/v一个/mq学习/gi机器/gi推广/gi未知/v数据/gi的/ude1能力/gi，/w即/v根据/p从/p样本数据/gb中/f学习/gi到/v的/ude1规则/gi能够/v应用/gi到/v新/a数据/gi的/ude1能力/gi。/w常用/a的/ude1计算/gi方法/gi是/vshi：/w用/p在/p训练/gi集/q上/f的/ude1误差/gi平均-在/nz测试/gi集/q上/f的/ude1误差/gi平均/a。/w一/nz：/w经验/gi风险/gi机器学习/gi本质/n上/f是/vshi一种/nz对/p问题/gi真实/a模型/gi的/ude1近/a，/w这种/r近/a模型/gi也/d叫做/v一个/mq假设/gi。/w因为/c真实/a模型/gi肯定/v是/vshi无法/v得到/v的/ude1，/w那/rzv我们/rr的/ude1假设/gi肯定/v与/cc真实情况/l之间/f存在/v误差/gi，/w这种/r误差/gi或者/c误差/gi的/ude1积累/gi也/d叫做/v风险/gi。/w在/p我们/rr选择/gi了/ule一个/mq假设/gi（/w或者/c获得/v一个/mq分类器/n）/w后/f，/w为了/p得到/v真实/a误差/gi的/ude1近/a，/w我们/rr用/p分类器/n在/p样本数据/gb上/f的/ude1分类/gi结果/n与/cc样本/gi本身/rz真实/a结果/n之间/f的/ude1差/a来/vf表示/v。/w这个/rz差/a叫做/v经验/gi风险/gi。/w以前/f机器学习/gi中/f经常/d通过/p经验/gi风险/gi的/ude1最小化/nz作为/p目标/gi,/nz但是/c后来/t发现/v很/d多/a分类/gi函数/gi在/p样本/gi集合/gi上/f能够/v很/d轻易/d的/ude1获得/v100%/nz的/ude1正确率/n,/nz但是/c在/p对/p真实/a数据/gi的/ude1分类/gi却/d很糟/nz。/w也/d表明/v了/ule这种/r分类/gi函数/gi推广/gi能力/gi（/w泛化/gi能力/gi）/w差/a。/w导致/gi这种/r现象/n的/ude1原因/n是/vshi：/w经验/gi风险/gi并不/d能够/v真正/d的/ude1近/a真实/a风险/gi,/nz因为/c样本/gi集合/gi的/ude1数目/gi相对于/nz真实世界/gi要/v分类/gi的/ude1数据/gi来说/uls就是/v九牛一毛/nz。/w之后/f统计学/gi中/f就/d引入/v了/ule泛化/gi误差/gi界/n的/ude1概念/gi。/w二/nz：/w泛化/gi误差/gi界/n泛化/gi误差/gi界/n刻画/v了/ule学习/gi算法/gi的/ude1经验/gi风险/gi与/cc期望/vn风险/gi之间/f偏差/n和/cc收敛/gi速度/n./nz真实/a的/ude1风险/gi应该/v由/p两/nz部分/n组成/gi：/w1/nz：/w经验/gi风险/gi,/nz代表/nnt分类器/n在/p给定/gi样本/gi上/f的/ude1误差/gi（/w可以/v精确/gi计算/gi）/w。/w2/nz：/w置信/v风险/gi,/nz代表/nnt我们/rr可以/v在/p多/a大程度/n上/f信任/vn分类器/n在/p未知/v数据/gi上/f的/ude1分类/gi结果/n（/w不/d可以/v精确/gi计算/gi）/w,/nz因为/c不/d可以/v精确/gi计算/gi,/nz所以/c只能/v给出/v一个/mq估计/gi区间/gi,/nz也/d因为/c这个/rz泛化/gi误差/gi只能/v给出/v一个/mq上界/n。/w /x与/cc置信/v风险/gi相关/vn的/ude1变量/gi有/vyou两/nz个/q：/w a/nz)/nz样本/gi数量/n,/nz样本/gi数量/n越大/d表明/v我们/rr的/ude1学习/gi结果/n正确/a的/ude1可能性/gi越大/d,/nz此时/r置信/v风险/gi越/d小/a。/w b/nz)/nz,/nz分类/gi函数/gi的/ude1vc/nz维/b越大/d,/nz推广/gi能力/gi越差/nz,/nz置信/v风险/gi越大/d。/w真实/a风险/gi /x≤/nz /x经验/gi风险/gi  /x置信/v风险/gi。/w现在/t统计/gi学习/gi的/ude1目标/gi就/d从/p经验/gi风险/gi最小化/nz变为/v经验/gi风险/gi与/cc置信/v风险/gi之/uzhi和/cc最小化/nz。/w