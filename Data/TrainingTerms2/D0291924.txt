#机器学习/gi：/w多变/z量线性/nz回归/gi
*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz注/v：/w本/rz系列/n博客/gi是/vshi博主/nz学习/gistanford/nz大学/gi andrew ng /nz教授/nnt的/ude1《/w机器学习/gi》/w课程/gi笔记/gi。/w博主/nz深感/v学/v过/uguo课程/gi后/f，/w不/d进行/vn总结/gi很容易/nz遗忘/v，/w根据/p课程/gi加上/v自己/rr对/p不/d明白/v问题/gi的/ude1补充/vn遂/d有此/r系列/n博客/gi。/w本/rz系列/n博客/gi包括/v线性回归/gi、/w逻辑回归/gi、/w神经网络/nz、/w机器学习/gi的/ude1应用/gi和/cc系统设计/gi、/w支持/v向/p量机/nz、/w聚类/gi、/w将/d维/b、/w异常检测/gi、/w推荐系统/gi及/cc大规模/b机器学习/gi等/udeng内容/gi。/w*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz多/a变量/gi线性回归/gi多维/gi特征/gi目前为止/i，/w我们/rr探讨/v了/ule单/b变量/gi（/w特征/gi）/w的/ude1回归/gi模型/gi，/w现在/t我们/rr对/p房价/n模型/gi增加/v更多/ad的/ude1特征/gi，/w如/v房间数/n楼层/n等/udeng，/w构成/v一个/mq含有/v多/a个/q变量/gi的/ude1模型/gi，/w模型/gi中的/v特征/gi为/p（/wx /nz,/nzx /nz,/nz.../w,/nzx /nz）/w。/w多变/z量/n梯度下降/gi与/cc单/b变量/gi线性回归/gi类/gi，/w在/p多/a变量/gi线性回归/gi中/f，/w我们/rr也/d构建/gi一个/mq代价/gi函数/gi，/w则/d这个/rz代价/gi函数/gi是/vshi所有/b建模/gi误差/gi的/ude1平方/q和/cc，/w即/v：/w其中/rz：/w我们/rr的/ude1目标/gi和/cc单/b变量/gi线性回归/gi问题/gi中/f一样/uyy，/w是/vshi要/v找出/v使得/vi代价/gi函数/gi最小/a的/ude1一系列/b参数/gi。/w /x多变/z量线性/nz回归/gi的/ude1批量/d梯度下降/gi算法/gi为/p：/w左边/f为/p单/b变量/gi学习/gi方法/gi，/w右边/f为/p多/a变量/gi学习/gi方法/gi。/w梯度下降/gi法/n实践/gi1/nz /x特征/gi缩/vi放在/v我们/rr面对/v多维/gi特征/gi问题/gi的/ude1时候/n，/w我们/rr要/v保证/v这些/rz特征/gi都/d具有/v相近/a的/ude1尺度/n，/w这/rzv将/d帮助/v梯度下降/gi算法/gi更快/d地/ude2收敛/gi。/w以/p房价/n问题/gi为/p例/n，/w假设/gi我们/rr使用/gi两/nz个/q特征/gi，/w房屋/n的/ude1尺寸/gi和/cc房间/n的/ude1数量/n，/w尺寸/gi的/ude1为/p /x0/nz-/nz2000/nz平方英尺/nz，/w而/cc房间/n数量/n的/ude1则/d是/vshi0/nz-/nz5/nz，/w以/p两/nz个/q参数/gi分别为/v横纵/nz坐标/gi，/w绘制/v代价/gi函数/gi的/ude1等高线图/n能/v，/w看出/v图像/gi会/v显得/v很扁/n，/w梯度下降/gi算法/gi需要/v非常/d多/a次/qv的/ude1迭代/gi才能/n收敛/gi。/w解决/v的/ude1方法/gi是/vshi尝试/v将/d所有/b特征/gi的/ude1尺度/n都/d尽量/d缩放/gi到/v-/nz1/nz /x到/v1/nz之间/f。/w如/v图/gi：/w最简单/nz的/ude1方法/gi是/vshi令/v：/w2/nz /x学习/gi率/v梯度下降/gi算法/gi收敛/gi所/usuo需要/v的/ude1迭代/gi次数/gi根据/p模型/gi的/ude1不同/a而/cc不同/a，/w我们/rr不能/v提前/vd预知/v，/w我们/rr可以/v绘制/v迭代/gi次数/gi和/cc代价/gi函数/gi的/ude1图表/gi来/vf观测/vn算法/gi在/p何时/ryt趋于/v收敛/gi。/w梯度下降/gi算法/gi的/ude1每次/r迭代/gi受到/v学习/gi率/v的/ude1影响/gi，/w如果/c学习/gi率/v /xα/nz过/uguo小/a，/w则/d达到/v收敛/gi所需/nz的/ude1迭代/gi次数/gi会/v非常/d高/a；/w如果/c学习/gi率/vα/nz /x过/uguo大/a，/w每次/r迭代/gi可能/v不会/v减小/v代价/gi函数/gi，/w可能会/nz越过/v局部/n最小/a导致/gi无法/v收敛/gi。/w通常/d可以/v考虑/v尝试/v些/q学习/gi率/v：/wα/nz=/nz0.01/nz，/w0.03/nz，/w0.1/nz，/w0.3/nz，/w1/nz，/w3/nz，/w10/nz特征/gi和/cc多项式/gi回归/gi如/v房价/n预测/gi问题/gi：/w线性回归/gi并不/d适用/vi于/p所有/b数据/gi，/w有时/d我们/rr需要/v曲线/gi来/vf适应/v我们/rr的/ude1数据/gi，/w通常/d我们/rr需要/v先/d观察/gi数据/gi然后/c再/d决定/v准备/v尝试/v怎样/ryv的/ude1模型/gi。/w另外/c，/w我们/rr可以/v将/d模型/gi转化/gi为/p线性回归/gi模型/gi。/w如下/vi图/gi x/nz-/nzsize/nz：/w注/v：/w如果/c我们/rr采用/v多项式/gi回归/gi模型/gi，/w在/p运行/gi梯度下降/gi算法/gi前/f，/w特征/gi缩放/gi非常/d有/vyou必要/a。/w正规/a方程/gi到/v目前为止/i，/w我们/rr都/d在/p使用/gi梯度下降/gi算法/gi，/w但是/c对于/p某些/rz线性回归/gi问题/gi，/w正规/a方程/gi方法/gi是/vshi更好/d的/ude1解决方案/gi，/w它/rr可以/v直接/ad解出/nz参数/gi。/w如/v：/w假设/gi我们/rr的/ude1训练/gi集/q特征/gi矩阵/gi为/px/nz（/w包含/v了/ule x/nz0/nz=/nz1/nz）/w并且/c我们/rr的/ude1训练/gi集/q结果/n为/p向量/gi y/nz，/w则/d利用/v正规/a方程/gi解出/nz向量/gi：/w以下/f表示/v数据/gi为/p例/n：/w则/d根据/p公式/gi：/w /x可以/v得到/v所需/nz参数/gi。/w梯度下降/gi与/cc正规/a方程/gi的/ude1比较/gi：/w*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz作者/nnt：/w时间/gi：/w2015/nz//nz8/nz//nz9/nz文章/gi地址/gi：/w*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz*/nz