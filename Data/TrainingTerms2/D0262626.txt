#白话/gi机器学习/gi算法/gi（/w六/nz）/w PCA/nz
是/vshi一种线/nz性/ng映射/gi，/w目的/gi是/vshi为了/p降维/nz。/w在/p机器学习/gi算法/gi中/f，/w映射/gi可以/v降维/nz也/d可以/v升高/v维度/gi，/w看/v最终/d的/ude1目的/gi是/vshi什么/ry，/w如果/c为了/p简化/gi输入/v特征/gi，/w可以/v降维/nz，/w如果/c数据/gi在/p低/a维线性/nz不可分/nz，/w可以/v考虑/v将/d数据/gi映射/gi到/v高/a维空间/nz中/f，/w那样/rzv可能/v就/d线性/gi可分/v了/ule，/w这/rzv就是/v核/n方法/gi。/wpca/gi属于/v前者/rzv；/w讲/vpca/gi之前/f我/rr要/v提/v几/d个/q概念/gi1/nz：/w投影/n2/nz：/w协方差/gm3/nz：/w正交/nz向量/gi对于/p1/nz，/w投影/n是/vshi一个/mq标量/n，/w如果/c我们/rr说/v a/nz在/pb/nz方向/gi上/f的/ude1投影/n，/w就是/v表示/v，/wa/nz中的/vb/nz分量/n的/ude1大小/n。/w这里/rzsb/nz仅仅/d代表/nnt方向/gi而/cc没有/v长度/gi，/wb/nz是/vshi一个/mq单位向量/gm；/w怎样/ryv算/v投影/n呢/y，/w就是/v求/v内积/gp！/w对于/p2/nz，/w一般/ad我们/rr知道/v对于/p单个/b随机变量/nz，/w有/vyou方差/gm这个/rz概念/gi，/w当/p把/pba单个/b随机变量/nz推广/gi到/v两/nz个/q的/ude1时候/n，/w就/d有/vyou了/ule协方差/gm；/w对于/p3/nz，/w空间/n可以/v看成/v是/vshi若/c干/v正交/nz向量/gi张成/nr的/ude1空间/n，/w该/rz空间/n上/f的/ude1任意/d一个/mq点/gi都/d可以/v用/p这些/rz正交/nz向量/gi加权/vn起来/vf；/w现在/t我们/rr有/vyou若/c干/v向量/gi，/w这些/rz向量/gi呢/y，/w可以/v看成/v是/vshi一个/mq高维/nz随机/b向量/gi的/ude1若/c干/v观测/vn，/w因而/c就/d有/vyou了/ule均/d，/w协方差/gm的/ude1概念/gi；/w假设/gi现在/t有/vyou个/q向量/gi集合/gi，/w每个/r向量/gi就是/v一个/mq观测/vn，/w有/vyoud/nz个/q维度/gi，/w我们/rr观测/vnm/nz次/qv /x；/w数据/gi就/d可以/v用/pd/nz乘以/vm/nz的/ude1矩阵/gi来/vf表示/v，/w矩阵/gi的/ude1一列/nz就是/v一个/mq观测/vn结果/n；/w现在/t用/p一个/mq映射/gi矩阵/gi左/f乘/v这个/rz数据/gi矩阵/gi，/w相当于/v将/d数据/gi矩阵/gi进行/vn了/ule映射/gi，/w得到/v原来/d数据/gi矩阵/gi在/p新/a空间/n上/f的/ude1投影/n，/w也/d就/d有/vyou原/b数据/gi在/p新的/a坐标/gi空间/n上/f的/ude1表示/v，/w这个/rz映射/gi的/ude1目的/gi是/vshi，/w使得/vi映射/gi结果/n属性/gi间/f无关/v，/w而且/c第一行/nz的/ude1方差/gm第二行/nz的/ude1方差/gm.../w./nz./nz最后/f一行/n的/ude1方差/gm；/w这么/rz做/v可以/v保留/v数据/gi的/ude1个性/n，/w剔除/v数据/gi的/ude1共性/n，/w去除/v属性/gi间/f的/ude1相关性/gi；/w去除/v属性/gi间/f的/ude1相关性/gi是/vshi通过/p /x映射/gi矩阵/gi每行/r的/ude1正交/nz性/ng获得/v；/w映射/gi矩阵/gi如何/ryv获得/v？/w对/p原/b数据/gi的/ude1协方/nz差矩阵/nz进行/vn特征/gi分解/gi，/w将/d获得/v的/ude1正交化/nz的/ude1特征向量/nz，/w按照/p对应/vi特征/gi由/p大/a到/v小/a按/p行/ng排列/gi就/d能/v得到/v；/wpca/gi的/ude1思想/gi就是/v坐标变换/gm，/w通过/p构建/gi一个/mq新的/a正交/nz空间/n，/w重新/d构建/gi原/b数据/gi，/w这个/rz思想/gi跟/p傅里叶变换/gi是/vshi一致/a的/ude1，/w说白了/l，/w就是/v换/v一种/nz表示/v方法/gi；/w如上图/i，/w原来/d用/px/nz，/wy/nz坐标/gi表示/v的/ude1点坐标/nz，/w我/rr可以/v用/pa/nz，/wb/nz坐标/gi来/vf表示/v，/w当/p我/rr需要/v降维/nz的/ude1情况下/nz，/w我/rr只用/va/nz坐标/gi就/d能/v大概/d表示/v这些/rz数据/gi的/ude1特征/gi，/w问题/gi就/d集中/v在/p怎么/ryv构建/gi这样/rzv一个/mq新的/a坐标系/gi了/ule。/w上面/f已经/d回答/v；/w具体/a的/ude1证明/v：/w方差/gm最大化/v（/w目标函数/gi）/w拉朗日/nz（/w略/d）/w总结/gi：/wpca/gi的/ude1目的/gi是/vshi用/p最少/d的/ude1维度/gi来/vf保留/v原/b数据/gi的/ude1最大/gm信息/gi，/w这种/r映射/gi对/p数据/gi的/ude1实际/n划分/v作用/gi并不大/v！/w实际上/d映射/gi以后/f，/w两点/nz在/p原/b空间/n的/ude1欧式/b距离/gi=/nz两/nz点/gi在/p新/a空间/n的/ude1欧式/b距离/gi。/w这/rzv是/vshi他/rr的/ude1原理/gi决定/v的/ude1，/w不能/v对/ppca/gi要求/n过/uguo高/a，/w或者/c把/pba它/rr用/p在/p错误/gi的/ude1地方/n！/w如果/c我们/rr需要/v一种/nz降维/nz以后/f两/nz类/gi间/f的/ude1距离/gi变得/vi更大/d了/ule，/w而/cc类/gi内/f距离/gi变/v紧凑/a了/ule，/w在线/vn性/ng方法/gi中/f我们/rr可以/v找/vlda/nz这位/r哥们/n，/w传说中/nz的/ude1fisher/nz判别分析/gc；/w最后/f：/w一位/nz面试官/nz问/v我/rrpca/gi与/ccgtm/nz的/ude1区别/gi，/w当时/t因为/c那天/r种种原因/n，/w未能/v答/v清楚/a，/w这里/rzs补充/vn下/f吧/y：/w两者/rzv从/p结果/n看/v都/d是/vshi映射/gi，/w都/d能降/v维度/gi，/w但是/c降维/nz的/ude1目的/gi不同/a，/w最终目标/nz空间/n意义/n也/d不同/a，/wpca/gi还是/c欧式/b空间/n的/ude1坐标变换/gm，/wgtm/nz是/vshi概率/gi空间/n的/ude1映射/gi，/wgtm/nz映射/gi后/f距离/gi近/a，/w说明/v两者/rzv属于/v同一/b类别/gi（/w即/v属于/v同一个/b高斯分布/gm）/w的/ude1概率/gi大/a，/wgtm/nz的/ude1映射/gi空间/n是/vshi人/n为/p定义/gi的/ude1，/w仅仅/d作为/p高维/nz数据可视化/gi的/ude1使用/gi，/wpca/gi则/d是/vshi赤裸裸/z的/ude1换/v个/q坐标/gi（/w角度/n）/w看/v数据/gi！/w以后/f博客/gi会/v详细/gi讲讲/vgtm/nz（/w生成/v拓扑/n映射/gi）/w！/w