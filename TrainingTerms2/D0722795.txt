#梯度下降/gi法/n的/ude1数学/gi推导/gi
第一次/nz接触/v梯度下降/gi法/n是从/vandrew ng/nz的/ude1机器学习/gi课程/gi上/f，/w当时/t看到/v这个/rz公式/gi有点/d疑惑/vn，/w为什么/ryv这样/rzv迭/d代几次/nz最后/f就/d能/v收敛/gi到/v局部极值/gi呢/y？/w /x其中/rz，/wα/nz称为/v学习/gi率/v，/w是/vshi一个/mq能/v自己/rr设定/v的/ude1常数/n，/w通常/d很小/a，/w下面/f还/d会/v讲到/v；/wθ/nz是/vshi各个/rz参数/gi的/ude1权重/nz（/w是/vshi一个/mq向量/gi）/w，/w因为/c我们/rr的/ude1目标/gi是/vshi确定/v一个/mqθ向量/nz使得/vi下式/n得到/v最小值/gm：/w /x这个/rz问题/gi的/ude1来源/gi可以/v参看/v这个/rz博客/gi（/w）/w /x这个/rz地方/n我/rr当时/t还/d疑惑/vn：/w直接/ad求导/nz让/v导数/n为/p“/w0/nz”/w不/d就/d能/v取得/v极值/gm么/y？/w学到/v后面/f发现/v思想/gi是/vshi对/p的/ude1，/w实际/n是/vshi不/d可行/a的/ude1，/w比如/v有时候/d求解/gi这样/rzv的/ude1方程/gi组/n非常复杂/b./nz /x回到/v最/d开始/v的/ude1问题/gi，/w为什么/ryv用/p梯度下降/gi法/n能/v收敛/gi到/v极值/gm呢/y？/w /x引用/gi文献/nstanford/nz机/ng局部优化/gc算法/gi之一/rz：/w /x梯度下降/gi法/n器/ng学习/gi―/w第一/mq讲/v./nz      /x梯度下降/gi法/n /x局部优化/gc算法/gi    /x李金屏/nz