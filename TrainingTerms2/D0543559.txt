#【/w机器学习/gi算法/gi】/w之/uzhilogistic/gi回归/gi
一/nz./nz算法/gi介绍/gi还是/c那句话/nz：/w统计/gi学习/gi=/nz模型/gi+/nz策略/gi+/nz算法/gi /x1./nz模型/gi logistic/nz模型/gi是/vshi对/p条件/n概率/gi进行/vn了/ule建模/gi：/w  /x虽然/c叫做/vlogistic/gi回归/gi，/w但/c实际上/d解决/v的/ude1是/vshi基本/a的/ude1二分类/nz问题/gi，/w因此/c可以/v建立/gi上述/b的/ude1条件/n概率模型/gi。/w  /x这里/rzs利用/v了/ulesigmoid/nz函数/gi的/ude1特性/gi，/w比/p之/uzhi线性/gi模型/gi，/w应当/v是/vshi一个/mq更加/d合理/a的/ude1模型/gi2./nz策略/gi /x在/p建立/gi好/a概率模型/gi之后/f，/w本/rz算法/gi使用/gi的/ude1策略/gi是/vshi最大/gm似/vg然/rz法则/gi，/w同样/d可以/v理解/gi成/v最小/a经验/gi风险/gi准则/n。/w给定/gi一个/mq训练/gi集/q（/wxi/nz,/nzyi/nz）/w，/w那么/c对于/p每个/rxi/nz都/d可以/v得到/vp/nz(/nzy/nz|/nzx/nz)/nz的/ude1概率/gi（/w由/p参数/githeat/nz表示/v）/w，/w把/pba他们/rr乘起来/l即可/v得到/v似然/nz表达式/gi，/w如下/vi所示/nz：/w  /x目标/gi就是/v最大化/v上述/b的/ude1似然/nz表达式/gi3./nz算法/gi sigmoid/nz函数/gi是/vshi一个/mq非线性函数/nz，/w上式/b没法/v求得/v闭式/n的/ude1最优解/gi。/w因此/c可以/v采用/v梯度/gi上升/vi算法/gi来/vf求解/gi最大值/gm，/w即/v如下/vi所示/nz：/w  /x在/p实际/n求解/gi中/f往往/d使用/gi随机/b梯度下降/gi法/n，/w关于/p随机/b梯度下降/gi和/cc批量/d梯度下降/gi，/w详见/nz：/w /x二/nz./nzpython/gi实现/gi上面/f的/ude1代码/gi片段/n使用/gi的/ude1是/vshi随机/b梯度下降/gi算法/gi，/w同时/c还/d对/p步长/nzalpha/nz做/v了/ule一/nz些/q处理/vn，/w会/v随着/p迭代/gi次数/gi的/ude1增加/v而/cc下降/vi。/w这/rzv也/d是/vshi一种/nz比较/gi好/a的/ude1做法/n，/w因为/c在/p实际/n的/ude1应用场景/gi下/f，/w往往/d会/v有/vyou个别/a难以/d正确/a分类/gi的/ude1数据/gi，/w因为/c这些/rz数据/gi而/cc大幅/d改变/v参数/gi是/vshi不合理/nz的/ude1。/w通过/p对/p步长/nz的/ude1修改/gi，/w使得/vi当迭/nz代/q次数/gi很大/d的/ude1时候/n，/w步长/nz变小/d，/w这样/rzv就/d会/v使得/vi参数/gi不会/v太/d受/v无法/v分类/gi点/gi的/ude1影响/gi。/w