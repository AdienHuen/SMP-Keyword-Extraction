#局部/n加权/vn回归/gi、/w逻辑斯/nz谛/nz回归/gi、/w感知器/nz算法―斯坦福/nzML/nz公开课/n笔记/gi3/nz
转载/v请/v注明/v：/w最近/t在/p看/vng/nz的/ude1机器学习/gi公开课/n，/wng/nz的/ude1讲法/n循循善诱/vl，/w感觉/gi提高/v了/ule不/d少/a。/w该/rz系列/n视频/gi共/d20/nz个/q，/w每/rz看完/v一个/mq视频/gi，/w我/rr都/d要/v记录/gi一/nz些/q笔记/gi，/w包括/v公式/gi的/ude1推导/gi，/w讲解/gi时候/n的/ude1例子/gi等/udeng。/w按照/png/nz的/ude1说法/n，/w公式/gi要/v自己/rr推理/vn一/nz遍/qv才能/n理解/gi的/ude1通透/nz，/w我/rr觉得/v自己/rr能够/v总结/gi出来/vf，/w发到/v博客/gi上/f，/w也/d能/v达到/v这个/rz效果/gi，/w希望/v有兴趣/v的/ude1同学/n要/v循序渐进/vl，/w理解/gi完/vi一个/mq算法/gi再/d开始/v学/v另外/c一个/mq算法/gi，/w每个/r算法/gi总结/gi一/nz遍/qv，/w虽然/c看起来/v很慢/d，/w但/c却/d真正/d的/ude1理解/gi了/ule，/w所谓/v虽/c慢实/nz快/a者/k也/d。/w该/rz系列/n的/ude1视频/gi对于/p数学公式/gm的/ude1推导/gi讲/v的/ude1很细/a，/w相信/v看完/v该/rz视频/gi后会/n对/p机器学习/gi的/ude1各种/rz算法/gi的/ude1推导/gi很/d熟悉/v。/w由于/pcsdn/gi博客/gi上/f写/v公式/gi实在/d是/vshi太难/d弄/v了/ule，/w如果/c一个/mq公式/gi一个/mq公式/gi的/ude1转成/v图片/gi传上来/v，/w反而/d是/vshi排版/vn很差/d。/w所以/c索性/d全部/m弄成/nz图片/gi传上来/v，/w虽然/c这样/rzv不利于/vseo/gi，/w但是/c在/pcsdn/gi这个/rz平台/gi下/f，/w相信/v还是/c会/v有/vyou很/d多/a人/n看到/v我/rr的/ude1博客/gi的/ude1，/w希望/v能/v对/p读者/n有所/v帮助/v。/w视频/gi1/nz-/nz2/nz的/ude1笔记见/nz，/w本文/r是/vshi第三/nz个/q视频/gi的/ude1笔记。第/nz3/nz个/q视频/gi的/ude1笔记/gi如下/vi，/w主要/b的/ude1内容/gi包括/v局部/n加权/vn回归/gi、/w最小/a二乘/nz的/ude1概率/gi解释/gi、/w逻辑斯蒂/nz回归/gi、/w感知器/nz算法/gi。/w