#机器学习/gi中/f关于/p判断/gi函数凸/nz或/c凹/a以及/cc最优化/gm的/ude1问题/gi
在/p很/d多/a机器学习/gi算法/gi中/f，/w都会/n遇到/v最优化/gm问题/gi。/w因为/c我们/rr机器学习/gi算法/gi，/w就是/v要/v在/p模型/gi空间/n中/f找到/v这样/rzv一个/mq模型/gi，/w使得/vi这个/rz模型/gi在/p一定/b范围内/nz具有/v最优/ad的/ude1性能/gi表现/v。/w因此/c，/w机器学习/gi离不开/v最优化/gm。/w然而/c，/w对于/p很/d多/a问题/gi，/w我们/rr并不/d总/b能够/v找到/v这个/rz最优/ad，/w很/d多/a时候/n我们/rr都/d是/vshi尽力/d去/vf找到/v近似/a最优/ad，/w这/rzv就是/v解析解/nz和/cc近似/a解/v的/ude1范畴/n。/w很/d多/a最优化/gm问题/gi都/d是/vshi在/p目标函数/gi是/vshi凸函数/gm或者/c凹函数/nz的/ude1基础上/nz进行/vn的/ude1。/w原因/n很/d简单/a，/w凸函数/gm的/ude1局部/n极小值/n就是/v其/rz全局/n最小值/gm，/w凹函数/nz的/ude1局部/n极大值/n就是/v其/rz全局/n最大值/gm。/w因此/c，/w只要/c我们/rr依据/n一个/mq策略/gi，/w一步步/nz地/ude2逼近/v这个/rz极值/gm，/w最终/d肯定/v能够/v到达/v全局/n最/d值/n附近/f。/w那么/c，/w如何/ryv判断/gi目标/gi函数凸/nz或者/c凹/a呢/y？/w判断/gi目标/gi函数凸/nz或者/c凹/a的/ude1方法/gi1/nz /x暴力/n计算法/nz这个/rz方法/gi是/vshi我/rr自己/rr起/vf的/ude1名字/n，/w哈哈/o，/w但是/c方法/gi不是/c我/rr发明/v的/ude1。/w。/w所谓/v暴力/n计算法/nz，/w就是/v直接/ad对/p目标函数/gi进行/vn计算/gi，/w然后/c判断/gi其/rz是否/v凸/ag。/w具体/a地/ude2，/w就是/v计算/gi目标函数/gi的/ude1一/nz阶导/nz数/n和/cc二阶导/nz数/n。/w然后/c作出/v判断/gi。/w凸函数/gm的/ude1一阶充/nz要/v条件/n等号/n右边/f是/vshi对/p函数/gi在/px/nz点/gi的/ude1一/nz阶/ng近似/a。/w这个/rz条件/n的/ude1意义/n是/vshi，/w对于/p函数/gi在/p定义/gi域/ng的/ude1任意/d取值/v，/w函数/gi的/ude1值/n都/d大于/v或者/c等于/v对/p函数/gi在/p这/rzv点/gi的/ude1一/nz阶/ng近似/a。/w用/p图/gi来/vf说明/v就是/v：/w通过/p图/gi可以/v很/d清楚/a地/ude2理解/gi这个/rz充要/a条件/n，/w但是/c，/w具体/a在/p应用/gi中/f，/w我们/rr不/d可能/v对/p每一个/nz点/gi都/d去/vf计算/gi函数/gi的/ude1一/nz阶/ng导数/n吧/y，/w因此/c下面/f这个/rz充要/a条件/n更加/d实用/a。/w凸函数/gm的/ude1二阶充/nz要/v条件/n很/d简单/a，/w如果/c一个/mq函数/gi的/ude1二阶/nr导数/n大于/v等于零/i，/w那么/c这个/rz函数/gi就是/v凸函数/gm。/w图/gi就/d不/d上/f了/ule，/w很好/ad理解/gi，/w函数/gi的/ude1一/nz阶导/nz数/n具有/v递增性/nz，/w那么/c函数/gi本身/rz就是/v凸函数/gm。/w通过/p暴力/n计算法/nz，/w可以/v很快/d地/ude2判断/gi函数/gi是不是/v凸函数/gm。/w凹函数/nz同理/v。/w2/nz /x结构/gi分析法/nz重要/a的/ude1内容/gi都/d在/p后面/f，/w哈哈/o。/w有时候/d我们/rr不必/d通过/p暴力/n计算/gi，/w可以/v通过/p分析/gi目标函数/gi的/ude1结构/gi，/w就/d能/v在/p一些/m情况下/nz判断/gi函数/gi是否是/l凸函数/gm。/w下面/f给出/v一/nz些/q结论/gi：/w指数/n函数/gi是/vshi凸函数/gm；/w对数函数/gm是/vshi凹函数/nz，/w然后/c负/v对数函数/gm就是/v凸函数/gm；/w对于/p一个/mq凸函数/gm进行/vn仿射变换/gg，/w可以/v理解/gi为/p线性变换/gm，/w结果/n还是/c凸函数/gm；/w二次函数/gm是/vshi凸函数/gm（/w二次项/nz系数/n为/p正/d）/w；/w高斯/nrf分布函数/nz是/vshi凹函数/nz；/w多/a个/q凸函数/gm的/ude1线性/gi加权/vn，/w如果/c权值/nz是/vshi大于/v等于零/i的/ude1，/w那么/c整个/b加权/vn结果/n函数/gi是/vshi凸函数/gm。/w下面/f出/vf一道/d题目/gi：/w如何/ryv判断/gi最大/gm似/vg然函数/nz一定/b有/vyou最大值/gm？/w思路/gi：/w最大/gm似/vg然函数/nz是/vshi求/v最大值/gm，/w那么/c函数/gi必须/d是/vshi凹函数/nz。/w就/d拿/v我们/rr常用/a的/ude1对数/gi似然/nz函数/gi，/w是/vshi多/a个/q对数函数/gm的/ude1线性/gi加权/vn而且/c权值/nz为/p1/nz，/w而/cc对数函数/gm是/vshi凹函数/nz，/w然后/c每个/r对数/gi内部/f有/vyou没有/v嵌套/v其他/rzv函数/gi再/d分析/gi一下/m，/w最后/f就/d能/v判断/gi整个/b对数/gi似然/nz函数/gi是/vshi凹函数/nz，/w因此/c一定/b有/vyou最大值/gm。/w机器学习/gi中的/v最优化/gm问题/gi很/d多/a机器学习/gi算法/gi都/d设计/gi最优化/gm问题/gi，/w判断/gi目标函数/gi是/vshi凸/ag是/vshi凹/a是/vshi第一步/nz，/w这/rzv只是/d可以/v最优化/gm的/ude1前提/n，/w那么/c，/w有/vyou哪些/ry最优化/gm的/ude1问题/gi呢/y？/w线性/gi规划/gi二次/nz规划/gi二次/nz约束/gi的/ude1二次/nz规划/gi半正定/nz规划/gi有/vyou哪些/ry最优化/gm的/ude1手段/n呢/y？/w常见/a的/ude1有/vyou：/w梯度/gi上升/vi（/w下降/vi）/w法/n牛顿法/nz /x//nz /x拟/v牛顿法/nz坐标/gi下降/vi法/n关于/p这/rzv部分/n的/ude1知识/gi下次/t详谈/v，/w先/d给出/v几/d篇/q相关/vn的/ude1资料/gi：/w  /x