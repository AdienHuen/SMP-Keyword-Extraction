#似然/nz函数/gi
在/p机器学习/gi的/ude1算法/gi中/f，/w我们/rr经常/d会/v见到/v“似/nz然函数/nz”/w这个/rz概念/gi。/w那么/c，/w什么/ry是/vshi“/w似/vg然函数/nz”/w呢/y？/w /x如果/c直接/ad看/v似然/nz的话/udh，/w比较/gi让/v人/n迷惑/v。/w但是/c如果/c看/v英文名/nz“/wlikelihood function /nz”/w，/w就/d可以/v大体/d明白/v，/w是/vshi表明/v“/w可能性/gi”/w的/ude1函数/gi。/w /x我们/rr知道/v，/w通常/d情况下/nz，/w我们/rr是/vshi利用/v“/w概率/gi”/w这个/rz词/n来/vf表名/nz可能性/gi的/ude1。/w比如/v：/w我们/rr知道/v抛/v一/nz枚/q硬币/n，/w其/rz正面/b朝/tg上/f的/ude1概率/gi为/pp/nz=/nz0.5。/nz那么/c，/w在/p我们/rr抛/v3/nz次/qv时/qt，/w其/rz全部/m朝/tg上/f的/ude1概率/gi是/vshi：/wp/nz=/nz0.5/nz*/nz0.5/nz*/nz0.5/nz /x,/nz也/d就是说/c，/w这个/rz3/nz次/qv全部/m向上/vi的/ude1可能性/gi是/vship/nz。/w /x那么/c似然/nz所/usuo对应/vi的/ude1可能性/gi是/vshi什么/ry呢/y？/w /x假设/gi另外/c一种/nz情况/n：/w当/p我们/rr连/ulian抛/v三次/nz，/w出现/v三次/nz都/d是/vshi向上/vi的/ude1时候/n，/w那/rzv我们/rr问/v，/w抛/v一次/nz，/w朝/tg上/f的/ude1概率/gi是/vshi多少/ry呢/y？/w或者说/c，/w抛/v一次/nz朝/tg上/f的/ude1可能性/gi是/vshi多/a大/a呢/y？/w此时/r的/ude1可能性/gi，/w指/v的/ude1就是/v似然/nz。/w而/cc最大/gm似然/nz，/w就是指/i把/pba这种/r可能性/gi进行/vn最大化/v。/w /x由此/d，/w我们/rr可以/v看到/v两者/rzv的/ude1一些/m差异/n：/w /x概率/gi，/w是/vshi用于/v在/p已知/nz一/nz些/q参数/gi的/ude1情况下/nz，/w预测/gi接下来/vl的/ude1观测所/n得到/v的/ude1结果/n；/w /x而/cc似然性/nz /x则/d是/vshi用于/v在/p已知/nz某些/rz观测所/n得到/v的/ude1结果/n时/qt，/w对/p有关/vn事物/n的/ude1性质/n的/ude1参数/gi进行/vn估计/gi。/w /x在/p这里/rzs，/w参数/gi可以/v是/vshi指/v单个/b的/ude1概率/gi，/w也/d可以/v指/v某些/rz像/vθ/nz\/nztheta/nz类/gi的/ude1参数/gi。/w