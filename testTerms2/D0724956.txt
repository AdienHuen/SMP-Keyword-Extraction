#机器学习/gi之/uzhi线性回归/gi
  /x根据/p网上/s某个/rz大牛/nz的/ude1说法/n，/w机器学习/gi的/ude1三个/nz要素/gi分为/v：/w /x模型/gi，/w策略/gi，/w /x算法/gi首先/d，/w对于/p一个/mq问题/gi，/w我们/rr用/p数学/gi语言/gi来/vf描述/gi它/rr，/w然后/c建立/gi一个/mq模型/gi，/w例如/v回归/gi模型/gi或者/c分类/gi模型/gi等/udeng来/vf描述/gi这个/rz问题/gi./nz其次/c，/w建立/gi求解/gi策略/gi，/w比如/v，/w通过/p最/d大然/nz估计/gi，/w最大/gm后/f验/v概率/gi，/w最小化/nz分类/gi误差/gi等等/udeng建立/gi模型/gi的/ude1代价/gi函数/gi，/w将/d其/rz转化/gi为/p一个/mq最优化/gm问题/gi./nz通过/p求解/gi这个/rz最优化/gm问题/gi，/w得到/v模型/gi的/ude1参数/gi。/w /x求解/gi最优化/gm问题/gi的/ude1策略/gi有/vyou，/w梯度下降/gi法/n等等/udeng。/w最后/f，/w确定/v求解/gi策略/gi之后/f，/w便是/v编程/gi实现/gi了/ule。/w线性回归/gi问题/gi首先/d建立/gi模型/gi：/w /x模型/gi很/d简单/a  /x然后/c是/vshi求解/gi策略/gi：/w /x建立/gi代价/gi函数/gi：/w  /x关于/p求解/gi策略/gi：/w   /x可以/v使用/gi梯度下降/gi法/n，/w批/q梯度下降/gi法/n，/w增量/gi梯度下降/gi。/w本质/n上/f，/w都/d是/vshi偏导/nz数/n，/w步长/nz//nz最佳/z学习/gi率/v，/w更新/gi，/w收敛/gi的/ude1问题/gi。/w /x