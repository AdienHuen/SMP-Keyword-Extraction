#经验/gi误差/gi（/w经验/gi风险/gi）/w最小化/nz
转载自/nz：/w前面/f提到/v，/w机器学习/gi的/ude1目的/gi就是/v根据/p一/nz些/q训练样本/n，/w寻找/v一个/mq最优/ad的/ude1函数/gi，/w使得/vi函数/gi对/p输入/vx/nz的/ude1估计/giy/nz'/nz与/cc实际/n输出/giy/nz之间/f的/ude1期望/vn风险/gi（/w可以/v暂时/d理解/gi为/p误差/gi）/w最小化/nz。/w期望/vn风险/gi最小化/nz依赖于/v样本/gi的/ude1输入/vx/nz与/cc其/rz输出/giy/nz之间/f的/ude1函数/gi映射/gi关系/gif/nz(/nzx/nz,/nzy/nz)/nz，/w而/cc这个/rz映射/gi关系/gi，/w在/p机器/gi视觉/n和/cc模式识别/gi系统/gi中/f，/w一般/ad指代/n先验概率/gm和/cc类/gi条件/n概率/gi。/w然而/c，/w这/rzv两者/rzv在/p实际/n的/ude1应用/gi中/f，/w都/d是/vshi无法/v准确/a获取/gi的/ude1，/w唯一/b能够/v利用/v的/ude1就/d只有/c训练样本/n的/ude1输入/vx/nz及其/cc对应/vi的/ude1观测/vn输出/giy/nz。/w而/cc机器学习/gi的/ude1目的/gi又/d必须/d要求/n使得/vi期望/vn风险/gi最小化/nz，/w从而/c得到/v需要/v的/ude1目标函数/gi。/w不难想象/l，/w可以/v利用/v样本/gi的/ude1算术/n平均/a来/vf代替/v式/k理想/n的/ude1期望/vn，/w于是/cc就/d定义/gi了/ule下面/f的/ude1式子/n来/vf作为/p实际/n的/ude1目标/gi风险/gi函数/gi是/vshi利用/v已知/nz的/ude1经验/gi数据/gi（/w训练样本/n）/w来/vf计算/gi，/w因此/c被/pbei称之为/v经验/gi风险/gi。/w用/p对/p参数/gi求/v经验/gi风险/gi来/vf逐渐/d近/a理想/n的/ude1期望/vn风险/gi的/ude1最小/a，/w就是/v我们/rr常/d说/v的/ude1经验/gi风险/gi最小化/nz（/werm/nz）/w原则/gi。/w显然/ad，/w利用/v经验/gi风险/gi来/vf代替/v真实/a的/ude1期望/vn风险/gi是/vshi有/vyou代价/gi的/ude1。/w首先/d，/w经验/gi风险/gi并不/d完全/ad等于/v期望/vn风险/gi；/w其次/c，/w用/p经验/gi风险/gi来/vf近/a代替/v期望/vn风险/gi在/p理论上/nz并/cc没有/v完善/v的/ude1依据/n；/w最后/f，/w用/p经验/gi风险/gi来/vf代替/v期望/vn风险/gi计算/gi得到/v的/ude1误差/gi属于/v经验/gi误差/gi，/w而/cc并非/v真实/a期望/vn误差/gi；/w尽管/c有/vyou这样/rzv那样/rzv的/ude1问题/gi存在/v，/w在/p先验概率/gm和/cc类/gi条件/n概率/gi无法/v准确/a获取/gi的/ude1情况下/nz，/w用/p经验/gi风险/gi来/vf“/w想当然/vl”/w的/ude1代替/v期望/vn风险/gi从而/c解决/v模式识别/gi等/udeng机器学习/gi问题/gi的/ude1思路/gi在/p这/rzv一/nz领域/gi依然/d大量/m存在/v。/w