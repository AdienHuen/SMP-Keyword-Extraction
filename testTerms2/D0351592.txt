#【/wPython/gi网络爬虫/gi】/w百度/ntc贴吧/gi//nz豆瓣/n小组/nis
为什么/ryv我/rr要/v做/v爬虫/gi其实/d做/v爬虫/gi已经/d有/vyou几次/nz经历/gi了/ule，/w但/c从来/d没有/v把/pba爬虫/gi的/ude1相关/vn工作/gi做/v过/uguo总结/gi，/w所以/c在/p我/rr第三次/nz写/v爬虫/gi却/d还要/d网页/gi搜索/gi具体/a写法/gi的/ude1时候/n，/w我/rr决定/v还是/c自己/rr把/pba爬虫/gi的/ude1相关/vn技术/gi记录下来/l。/w基础/gi版/n爬虫/gi最/d基础/gi版/n的/ude1爬虫/gi很/d简单/a，/w我们/rr都/d知道/v爬虫/gi其实/d就/d只是/d要/v把/pba网页/gi信息/gi获取/gi，/w也/d就是/v你/rr右键/gi看到/v的/ude1那/rzv堆/gi源代码/gi，/w然后/c按照/p自己/rr所/usuo需要/v的/ude1利用/v正则/n匹配/gi出/vf你/rr要/v抠/v的/ude1内容/gi就/d好/a了/ule，/w所以/c最/d基础/gi版/n的/ude1爬虫/gi作用/gi就/d在于/v此/rzs。/w /x人生/n苦短/v，/w我们/rr用/ppython/gi。/wpython/gi作为/p世界/gi上/f我/rr最喜欢/nz的/ude1编程语言/gi，/w也/d有/vyou很/d强大/a的/ude1网络/gi工具库/nz，/w在/ppython/gi2.7/nz中/f有/vyou利器/nurllib/nz2/nz来/vf帮助/v我们/rr实现/gi快速/d爬虫/gi。/w爬虫/gi的/ude1两/nz种/q方式/n：/w深度/gi优先/vd：/w先/d爬取/nz该/rz网页/gi的/ude1所有/b链接/gi再/d把/pba这些/rz链接/gi中的/v所有/b链接/gi都/d获取/gi了/ule，/w最后/f在/p爬/v内容/gi；/w广度优先/gi：/w先/d爬/v取/v某/rz网页/gi的/ude1链接/gi，/w把/pba该/rz网页/gi爬/v完了/vi再/d去/vf爬下/nz一个/mq网页/gi的/ude1内容/gi及/cc里面/f的/ude1链接/gi伪装成/n浏览器/gi进行/vn爬虫/gi为什么/ryv需要/v伪装/v因为/c好/a多/a网页/gi都/d设置/gi了/ule反爬虫/gi=/nz。/w=/nz一旦/d被/pbei发现/v是/vshi机器/gi而/cc不是/c人/n的话/udh就/d被/pbei被/pbei禁止/v访问/gi或者/c禁/v号/q的/ude1。/w怎么/ryv伪装/v服务器/gi一般/ad确定/v到底/d是/vshi机器/gi还是/c人/n在/p操作/gi是/vshi看/v发送数据/n的/ude1时候/n发送/gi的/ude1信息/gi里/f有/vyou没有/v他/rr想要/v查/v的/ude1数据/gi，/w比如/vuser/gi-/nzagent/nz，/w比如/vheader/nz。/w其中/rz这两项/nz也/d是/vshi最重要/nz的/ude1两项/nz，/w下面/f介绍/gi如何/ryv获取/gi。/w /x-/nz /x如何/ryv获取/gi浏览器/giheaders/nz信息/gi     /x采用/vdebug/gi的/ude1方式/n可以/v将/d爬取/nz的/ude1信息/gi最后/f打印/v出/vf的/ude1信息/gi如下/vi：/w /x通过/p浏览器/gi获取/gi /x很/d简单/a浏览器/gi上/ff/nz12/nz，/w然后/c点击/vnetwork/nz，/w刷新/v一/nz下/f界面/gi就/d可以/v看到/v你/rr给/p服务器/gi发/v了/ule什么/ry信息/gi，/w服务器返回/n了/ule什么/ry信息/gi。/w注意/v我/rr上面/f黑色/n拉/v出来/vf的/ude1一段/nz就是/v需要/v的/ude1user/gi-/nzagent/nz信息/gi伪装/v之后/f需要/v做/v什么/ry /x爬/v网页/gi+/nz正则/n神器/nz！/w /x关于/p正则表达式/gi的/ude1内容/gi不在/v这个/rz地方/n赘述/vi了/ule，/w下次/t单独/d写/v一篇/nz关于/p正则表达式/gi的/ude1内容/gi。/w爬虫/gi可以/v使用/gi的/ude1小/atricks/nz采用/vsleep/nz减慢/v你/rr爬虫/gi的/ude1速度/n，/w虽然/c会/v相对/d减慢/v速度/n，/w但/c这/rzv真的/d好/a过/uguo你/rr一次一次/nz的/ude1被/pbei封号/n又/d重新/dopen/nz你/rr的/ude1程序/gi更/d绝一点/nz就/d直接/ad把/pbasleep/nz的/ude1参数设置/n为/p一个/mq随机/b数/n=/nz。/w=/nz尽量/d爬一个/nz网页/gi就/d保存/gi到/v文本/gi里/f，/w要/v不/d你/rr会/v后悔/v的/ude1你/rr真的/d会/v后悔/v的/ude1。/w别问/nz我/rr怎么/ryv知道/v~/nz最后/f，/w给出/v两/nz个/q我/rr在/p项目/gi中用/a的/ude1代码/gi，/w其实/d对于/p爬虫/gi而言/uls最/d有/vyou特点/n的/ude1是/vshi每个/r网站/gi都/d不/d一样/uyy，/w所以/c在/p你/rr确定/v要/v爬虫/gi之前/f最好/d先/d解析/gi一下/m网页/gi的/ude1源代码/gi，/w看看/v好不好/l爬/v，/wthat/nz’/ws it/nz!/nz附录/n百度/ntc贴吧/gi爬虫/gi源代码/gi豆瓣/n小组/nis爬虫/gi源代码/gi