#如何/ryv提高/v机器学习/gi中的/v分类/gi准确率/gi
1./nz扩大/v数据/gi集/q。/w俗话说/vl的/ude1好/a，/w更好/d的/ude1数据/gi往往/d能/v打败/v更好/d的/ude1算法/gi。/w当/p我们/rr想要/v提高/v机器学习/gi的/ude1分类/gi准确率/gi时/qt，/w第一个/gi可用/v的/ude1方法/gi就是/v扩大/v数据/gi集/q。/w只要/c机器学习/gi花费/v的/ude1时间/gi在/p可以/v接受/gi的/ude1范围内/nz，/w就/d可以/v继续/v扩大/v数据/gi集/q，/w它/rr往往/d可以/v使/v我们/rr获得/v更/d理想/n的/ude1分类/gi准确率/gi。/w2./nz分类器/n选择/gi。/w遗憾/gi的/ude1是/vshi，/w理想/n的/ude1数据/gi集/q规模/gi往往/d是/vshi我们/rr可望不可即/i的/ude1。/w这时/rzt，/w我们/rr应该/v想到/v的/ude1就是/v选择/gi适合/v的/ude1分类器/n。/w以/pweka/nz为/p例/n，/w如果/c你/rr使用/gi的/ude1训练/gi集/q较小/d，/w那么/c高/a偏差/n//nz低/a方差/gm分类器/n（/w如/v朴素贝叶斯/gi）/w将/d会/v是/vshi你/rr较优/nz的/ude1选择/gi。/w然而/c，/w随着/p训练/gi集/q的/ude1增大/v，/w低/a偏差/n//nz高/a方差/gm分类器/n（/w如/vk/nz近邻/gi）/w将/d提供/v更好/d的/ude1分类/gi结果/n，/w因为/c它们/rr具有/v较低/d的/ude1渐近/nz误差/gi，/w而/cc高/a偏差/n分类器/n则/d不足以/v提供/v准确/a的/ude1模型/gi。/w此外/c，/w数据/gi的/ude1各个/rz属性/gi是/vshi离散/gi还是/c连续/gi，/w数据/gi噪声/n的/ude1大小/n等/udeng都/d可以/v成为/v选择/gi分类器/n的/ude1依据/n。/w3./nz属性子/nz集/q选择/gi。/w好马/nz需/v配/v好/a鞍/ng。/w在/p我们/rr选择/gi了/ule合适/a的/ude1分类器/n后/f，/w还/d需要/v进行/vn的/ude1一项/nz工作/gi就是/v数据/gi属性子/nz集/q的/ude1选取/gi，/w也/d就是/v选出/v那些/rz与/cc类别/gi属性/gi相关性/gi较强/d的/ude1，/w去除/v不/d相关/vn和/cc冗余/n的/ude1属性/gi。/w这项/r工作/gi如果/c人工/b来/vf做/v的话/udh，/w是/vshi十分复杂/nz且/c困难/an的/ude1。/w不过/c，/w好/a在/p很/d多/a机器学习/gi工具/gi都/d为/p我们/rr提供/v了/ule这项/r功能/gi，/w还以/cweka/nz为/p例/n，/w它/rr为/p我们/rr提供/v了/ule一个/mq工具/gi―/w―/w属性/gi评估器/n。/w属性/gi评估器/n分类/gi两/nz类/gi：/w一是/nz属性子/nz集/q评估器/n，/w它/rr的/ude1作用/gi是/vshi为/p我们/rr返回/v一个/mq“/w最/d优”/nz（/w是否/v真的/d最优/ad，/w还/d需/v进行/vn验证/v）/w的/ude1属性子/nz集/q。/w二是/nz单个/b属性/gi评估器/n，/w它/rr通过/p将/d给定/gi数目/gi的/ude1属性/gi进行/vn排序/gi，/w得到/v了/ule一个/mq直观/a的/ude1属性/gi排名/gi列表/vi，/w我们/rr可以/v自己/rr把/pba那些/rz排名/gi靠/v前/f的/ude1属性/gi选/v出来/vf，/w并用/vi它们/rr进行/vn分类/gi，/w从而/c提高/v准确率/gi。/w4./nz集成/vn学习/gi。/w俗话说/vl的/ude1好/a，/w三个臭皮匠/nz赛过诸葛亮/nz。/w机器学习/gi也/d是/vshi如此/rzv，/w集成/vn学习/gi的/ude1目标/gi就是/v通过/p把/pba若/c干/v个/q弱/a学习器/nz组合/gi起来/vf，/w使/v其/rz成为/v一个/mq强/a学习器/nz，/w从而/c获得/v更好/d的/ude1分类/gi准确率/gi。/w关于/p强/a、/w弱/a学习器/nz的/ude1定义/gi，/w大家/rr有兴趣/v可以/v百度/ntc，/w我/rr就/d不/d赘述/vi了/ule。/w然后/c，/w关于/p在/p编程/gi中/f如何/ryv实现/gi集成/vn学习/gi，/w我/rr给/p大家/rr一个/mq传送门/nz：/w。/w最后/f，/w我/rr再/d强调/v一点/m，/w在/p集成/vn的/ude1过程/gi中/f，/w选择/gi哪些/ry弱/a学习器/nz进行/vn集成/vn呢/y？/w有/vyou个/q判断/gi标准/gi，/w一/nz是/vshi弱/a学习器/nz的/ude1准确率/gi越高/d越好/d，/w二是/nz这些/rz弱/a学习器/nz之间/f的/ude1差异性/nz越大/d越好/d。/w根据/p这/rzv两/nz个/q标准/gi进行/vn选择性/n集成/vn将/d会/v获得/v很/d不错/a的/ude1效果/gi。/w5./nz算法/gi改进/gi。/w这/rzv点/gi非常/d难/a，/w尤其/d是/vshi在/p机器学习/gi已经/d发展/gi了/ule那么/c多/a年/qt，/w无/v数/n大牛/nz先后/d投入/v其中/rz的/ude1情况下/nz，/w想要/v改进/gi算法/gi是/vshi我们/rr一般/ad人/n做/v不到/v的/ude1。/w但是/c，/w也/d存在/v一种/nz可能/v，/w就是/v当/p我们/rr利用/v机器学习/gi解决/v某个/rz特定/b领域/gi的/ude1问题/gi时/qt，/w也许/d可以/v利用/v我们/rr对/p该/rz领域/gi知识/gi的/ude1掌握/v，/w比如/v该/rz领域/gi数据/gi有/vyou何/ry特点/n，/w来/vf对/p某/rz一算法/nz进行/vn适当/a的/ude1调整/vn，/w以/p使得/vi该/rz算法/gi可以/v更好/d地/ude2应用/gi在/p这个/rz领域/gi的/ude1数据/gi上/f，/w从而/c获得/v更好/d的/ude1效果/gi。/w如果/c有/vyou哪位/nz朋友/n有/vyou相关/vn的/ude1工作/gi经历/gi或者/c经验/gi，/w也/d希望/v您/rr能/v和/cc大家/rr分享/gi一下/m。/w以上/f只是/d我/rr刚刚/d接触/v机器学习/gi后/f的/ude1一/nz点点/v总结/gi，/w肯定/v存在/v很/d多/a错误/gi，/w还/d希望/v大家/rr多多/d批评指正/nz。/w